<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Methods · BLO Hub</title>
  <meta name="description" content="Solution methods for bilevel optimization: classical, evolutionary algorithms, ML-based methods, and hybrids." />

  <!-- MathJax (optional for inline math, not required for this page but handy) -->
  <script>
    window.MathJax = { tex: { tags: 'ams', tagSide: 'right', tagIndent: '0.8em' } };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- <link rel="stylesheet" href="assets/style.css" /> -->
</head>

<body>
  <header class="topbar">
    <div class="container topbar-inner">
      <a class="brand" href="index.html">BLO Hub</a>
      <nav class="nav">
        <a class="navlink" href="index.html">Home</a>
        <a class="navlink" href="taxonomy.html">Taxonomy</a>
        <a class="navlink" href="formulation.html">Formulations</a>
        <a class="navlink active" href="methods.html">Methods</a>
        <a class="navlink" href="papers.html">Papers</a>
        <a class="navlink" href="#references">References</a>
      </nav>
      <button class="menuBtn" id="menuBtn" aria-label="Toggle menu">☰</button>
    </div>

    <div class="mobileNav container" id="mobileNav">
      <a href="index.html">Home</a>
      <a class="navlink" href="taxonomy.html">Taxonomy</a>
      <a href="formulation.html">Formulations</a>
      <a href="methods.html">Methods</a>
      <a href="papers.html">Papers</a>
      <a href="#references">References</a>
    </div>
  </header>

  <main class="container section">
    <div class="pageHead">
      <div>
        <h1>Solution Methods</h1>
        <p class="muted">
          A practical organization of BLO solution approaches: <strong>Classical</strong>, <strong>Evolutionary Algorithms</strong>,
          <strong>Machine Learning</strong>, and <strong>Hybrid EA–ML</strong>.
        </p>
      </div>
      <div class="rightTools">
        <a class="btn" href="papers.html">Browse papers</a>
      </div>
    </div>

    <div class="grid2">
      <div class="card">
        <h2 id="classical">Classical methods</h2>
        <p>
          Classical approaches exploit analytical structure to reduce bilevel problems to single-level forms or to solve them exactly in
          restricted settings. They are most effective when the problem has a clear mathematical form (e.g., linear/convex/quadratic),
          with assumptions such as continuity/differentiability and regularity conditions at the lower level
          <a href="#ref-anandalingam1992hierarchical" class="cite">[3]</a>,
          <a href="#ref-colson2007overview" class="cite">[4]</a>.
        </p>
         <p class="muted small">
          Practical note: if the LL is nonconvex, discrete, nonsmooth, or available only through a simulator/legacy solver, classical guarantees may not apply,
          and one typically needs approximations (e.g., penalties, trust regions, or zeroth-order hypergradients) or derivative-free methods.
        </p>

        <hr/>

        <h3>Single-level reduction via KKT conditions (KKT/MPEC)</h3>
        <p>
          When the LL problem is convex and satisfies constraint qualifications, the LL can be replaced by its KKT optimality conditions.
          This yields a single-level formulation (often an MPEC), but introduces complementarity constraints that can be difficult to handle
          <a href="#ref-sinha2018review" class="cite">[1]</a>,
          <a href="#ref-bard1998practical" class="cite">[5]</a>.
        </p>

        <div class="equation"><div class="math">
\begin{equation}\label{eq:classical-ll}
\begin{aligned}
\mathbf{y}^*(\mathbf{x}) \in \arg\min_{\mathbf{y}} \quad & f(\mathbf{x},\mathbf{y})\\
\text{s.t.}\quad & g(\mathbf{x},\mathbf{y})\le 0.
\end{aligned}
\end{equation}
        </div></div>

        <p class="muted small"><strong>KKT replacement (typical):</strong></p>
        <div class="equation"><div class="math">
\begin{equation}\label{eq:classical-kkt}
\begin{aligned}
\nabla_{\mathbf{y}} f(\mathbf{x},\mathbf{y}) + \nabla_{\mathbf{y}} g(\mathbf{x},\mathbf{y})^\top \boldsymbol{\lambda} &= 0,\\
g(\mathbf{x},\mathbf{y}) \le 0,\quad \boldsymbol{\lambda}\ge 0,\quad 
\boldsymbol{\lambda}\odot g(\mathbf{x},\mathbf{y}) &= 0.
\end{aligned}
\end{equation}
        </div></div>

        <p class="muted small">
          In linear bilevel classes, this pathway often leads to MILP-style reformulations (e.g., via linearization of complementarity),
          which can then be solved with branch-and-bound (B&B) on small-to-medium structured instances
          <a href="#ref-bard1998practical" class="cite">[5]</a>.
        </p>

        <hr/>

        <h3>Descent methods (hypergradient-based classical schemes)</h3>
        <p>
          Descent methods seek directions that improve the UL objective while respecting that the LL must remain optimal.
          In smooth settings (e.g., nonconvex-strongly-convex bilevel), hypergradients can be obtained by implicit differentiation or by
          controlled approximation of the LL solution mapping
          <a href="#ref-ghadimi2018approximation" class="cite">[6]</a>.
        </p>

        <div class="equation"><div class="math">
\begin{equation}\label{eq:classical-hypergradient}
\begin{aligned}
\nabla_{\mathbf{x}} \,F(\mathbf{x},\mathbf{y}^*(\mathbf{x}))
&= \nabla_{\mathbf{x}}F(\mathbf{x},\mathbf{y})
+ \nabla_{\mathbf{y}}F(\mathbf{x},\mathbf{y}) \cdot \frac{d\mathbf{y}^*(\mathbf{x})}{d\mathbf{x}},\\[2pt]
\frac{d\mathbf{y}^*(\mathbf{x})}{d\mathbf{x}}
&= - \left[\nabla_{\mathbf{y}\mathbf{y}}^2 f(\mathbf{x},\mathbf{y}^*)\right]^{-1}\nabla_{\mathbf{x}\mathbf{y}}^2 f(\mathbf{x},\mathbf{y}^*) \quad (\text{typical strong-convex LL}).
\end{aligned}
\end{equation}
        </div></div>

        <p class="muted small">
          These ideas connect directly to modern differentiable BLO (implicit gradients/unrolling),
          but classical variants emphasize algorithmic safeguards (feasibility maintenance, subproblem checks, and step control).
        </p>

        <hr/>

        <h3>Trust-region methods</h3>
        <p>
          Trust-region strategies build local models (approximations) of the bilevel objective inside a neighborhood (“trust region”),
          expanding or shrinking the region depending on how accurately the local model predicts real improvement
          <a href="#ref-yuan2015trust" class="cite">[12]</a>.
          In bilevel settings, trust-region ideas can be combined with variational inequality (VI) views of the LL or stabilized approximations of the LL response
          <a href="#ref-colson2005trustregion" class="cite">[13]</a>,
          <a href="#ref-colson2007overview" class="cite">[4]</a>.
        </p>

        <hr/>

        <h3>Penalty function methods</h3>
        <p>
          Penalty methods incorporate LL constraints or optimality conditions into the UL objective using penalty terms,
          creating a single-level approximation solvable by standard constrained optimization toolchains.
          Modern first-order penalty frameworks for bilevel optimization provide convergence guarantees under smoothness assumptions
          <a href="#ref-lu2023penalty" class="cite">[9]</a>,
          while early bilevel penalty ideas appear in Stackelberg formulations and double-penalty constructions
          <a href="#ref-aiyoshi1984penalty" class="cite">[10]</a>,
          <a href="#ref-ishizuka1992double" class="cite">[11]</a>.
        </p>

        <div class="equation"><div class="math">
\begin{equation}\label{eq:classical-penalty}
\min_{\mathbf{x},\mathbf{y}} \quad F(\mathbf{x},\mathbf{y}) \;+\; \rho \,\Phi(\mathbf{x},\mathbf{y}),
\end{equation}
        </div></div>
        <p class="muted small">
          where \(\Phi\) penalizes LL infeasibility and/or LL suboptimality (or KKT residuals), and \(\rho\) is increased to enforce the LL conditions.
        </p>

        <hr/>

        <h3>Zeroth-order (black-box) bilevel methods</h3>
        <p>
          In many practical BLO settings (e.g., simulators, privacy-restricted learning, legacy solvers), gradients are unavailable and only noisy
          function evaluations of \(F(\mathbf{x},\mathbf{y})\) and \(f(\mathbf{x},\mathbf{y})\) can be queried.
          Recent fully zeroth-order bilevel methods estimate (hyper)gradients using smoothing-based estimators and then perform principled bilevel updates
          with oracle-complexity guarantees in smooth regimes
          <a href="#ref-aghasi2025fully" class="cite">[7]</a>,
          <a href="#ref-aghasi2025optimal" class="cite">[8]</a>.
        </p>

        <p class="muted small"><strong>Typical smoothing estimator (illustrative):</strong></p>
        <div class="equation"><div class="math">
\begin{equation}\label{eq:classical-zo}
\widehat{\nabla} F(\mathbf{x})
= \frac{d}{\mu}\left(F(\mathbf{x}+\mu \mathbf{u})-F(\mathbf{x})\right)\mathbf{u},
\qquad \mathbf{u}\sim \mathcal{N}(0,I),
\end{equation}
        </div></div>

        <p class="muted small">
          These methods complement EA-style black-box solvers by adding theory in smooth settings; EAs can remain preferable when the landscape is nonsmooth,
          discrete, heavily constrained, or highly rugged.
        </p>
        <p class="muted small">
          Strength: strong guarantees on structured instances. Weakness: can fail or become impractical with nonconvexity, nonsmoothness, discreteness, or black-box LL solvers.
        </p>
      </div>

     <div class="card">
  <h2 id="ea">Evolutionary algorithms (EAs)</h2>

  <p>
    Due to the limitations of classical approaches, evolutionary algorithms (EAs) have become widely used for BLO
    <a href="#ref-turgut2023systematic" class="cite">[14]</a>,
    <a href="#ref-chauhan2024competitive" class="cite">[15]</a>.
    These population-based, derivative-free methods are particularly effective for non-convex, multi-modal, noisy, or black-box settings
    <a href="#ref-7942105" class="cite">[1]</a>,
    <a href="#ref-10121701" class="cite">[16]</a>.
    EA-based BLO methods differ mainly in how they (i) handle the LL optimization, (ii) reuse structure or information across levels, and
    (iii) control the computational cost of nested evaluations.
  </p>

  <hr/>

  <h3>Taxonomy and common EA frameworks for BLO</h3>
  <p class="muted">
    The central difficulty is the LL “response mapping”: each UL candidate may require solving one (or many) LL problems. EA frameworks
    trade off generality vs. computational cost by choosing how explicitly they solve, approximate, or co-evolve the LL response.
  </p>

  <h4>1) Nested EA</h4>
  <p>
    <strong>Idea:</strong> run an EA at the UL, and for each UL candidate, run a full EA at the LL to compute the follower response
    <a href="#ref-sinha2014finding" class="cite">[17]</a>,
    <a href="#ref-kuo2011hybrid" class="cite">[18]</a>.
  </p>
  <ul class="bullets">
    <li><strong>LL handling:</strong> explicit LL optimization (EA inside EA).</li>
    <li><strong>Strength:</strong> fully derivative-free; works for black-box/simulation-based LLs.</li>
    <li><strong>Limitation:</strong> extremely expensive, LL evaluations dominate runtime, especially at high dimensions or costly simulations
      <a href="#ref-li2024bridging" class="cite">[19]</a>,
      <a href="#ref-acm-3638530-3648409" class="cite">[20]</a>.
    </li>
  </ul>

  <h4>2) Single-level reduction + EA</h4>
  <p>
    <strong>Idea:</strong> reduce the bilevel model to a single-level constrained problem (often via KKT/MPEC-style conditions) and then solve it with EAs
    <a href="#ref-li2023ga" class="cite">[21]</a>,
    <a href="#ref-camacho2022tabu" class="cite">[22]</a>,
    <a href="#ref-10899879" class="cite">[23]</a>.
    This can improve efficiency but relies on assumptions (e.g., convexity, smoothness, valid constraint qualifications), which may not hold in noisy or
    simulator-based settings
    <a href="#ref-yang2024bilevel" class="cite">[24]</a>,
    <a href="#ref-lachhwani2022comprehensive" class="cite">[25]</a>.
  </p>
  <ul class="bullets">
    <li><strong>LL handling:</strong> implicit (encoded as constraints/conditions).</li>
    <li><strong>Strength:</strong> avoids repeated LL solves; often faster when assumptions hold.</li>
    <li><strong>Limitation:</strong> reformulations can be brittle; complementarity constraints can be hard to handle robustly.</li>
  </ul>

  <h4>3) Co-evolutionary EAs</h4>
  <p>
    <strong>Idea:</strong> evolve two interacting populations simultaneously, one for UL (leader) solutions and one for LL (follower) responses, so both levels
    adapt together
    <a href="#ref-chaabani2018new" class="cite">[26]</a>,
    <a href="#ref-8425468" class="cite">[27]</a>.
    Co-evolution can explore multiple follower responses without fully solving the LL from scratch for every UL candidate.
  </p>
  <ul class="bullets">
    <li><strong>LL handling:</strong> approximate via an evolving follower population.</li>
    <li><strong>Strength:</strong> parallelizable; can maintain diverse LL responses (helpful when LL optima are non-unique).</li>
    <li><strong>Limitation:</strong> coordination/synchronization matters, poor coupling may cause instability or premature convergence.</li>
  </ul>

  <h4>4) Surrogate-assisted EAs</h4>
  <p>
    <strong>Idea:</strong> learn surrogate models (e.g., GP/RBF/NN) to approximate expensive LL evaluations (objective/constraints or response mapping), and use
    surrogates during the EA search to reduce cost
    <a href="#ref-islam2017surrogate" class="cite">[28]</a>.
    Reliability depends on surrogate fidelity; practical designs often add uncertainty-aware sampling and periodic true re-evaluation.
  </p>
  <p class="muted small">
    <strong>Remark:</strong> Surrogate-assisted EAs sit at the interface of EA and ML. The surrogate is an ML component, but the overall solver is still
    evolutionary, so many surveys categorize them under EA-based BLO (and sometimes also as early EA-ML hybrids)
    <a href="#ref-colson2007overview" class="cite">[4]</a>,
    <a href="#ref-dempe2002foundations" class="cite">[29]</a>.
  </p>

  <hr/>

  <h3>Representative evolutionary bilevel advances (post-2018)</h3>
  <p>
    Recent EA research emphasizes: (i) better scalability to higher-dimensional BLO, (ii) reducing nested LL cost via reuse/transfer/parallelism, and
    (iii) surrogate learning or candidate preselection for expensive LL responses. Representative directions include response-approximation ideas
    (e.g., BLEAQ)
    <a href="#ref-sinha2017_bleaq" class="cite">[30]</a>,
    CMA-ES-style bilevel search with distribution sharing
    <a href="#ref-8388227" class="cite">[31]</a>,
    and transfer-learning-based parallel bilevel EAs (e.g., TLEA)
    <a href="#ref-9476019" class="cite">[32]</a>.
    Multiobjective and cooperative variants have also been explored
    <a href="#ref-9721406" class="cite">[33]</a>.
  </p>

  <hr/>

  <h3>Strengths and limitations</h3>
  <ul class="bullets">
    <li><strong>Strengths:</strong> derivative-free; handles discrete/mixed variables; robust to nonconvexity and black-box evaluations; easy to parallelize.</li>
    <li><strong>Limitations:</strong> high cost in nested settings; sample-inefficiency; sensitivity to parameters; scaling challenges in high dimensions; LL non-uniqueness can cause unstable UL decisions.</li>
  </ul>

  <hr/>

  <h3>Summary table</h3>
  <div style="overflow:auto">
    <table style="width:100%; border-collapse:collapse">
      <thead>
        <tr>
          <th style="text-align:left; padding:8px; border-bottom:1px solid rgba(0,0,0,0.12)">Approach</th>
          <th style="text-align:left; padding:8px; border-bottom:1px solid rgba(0,0,0,0.12)">LL handling</th>
          <th style="text-align:left; padding:8px; border-bottom:1px solid rgba(0,0,0,0.12)">Scalability</th>
          <th style="text-align:left; padding:8px; border-bottom:1px solid rgba(0,0,0,0.12)">Gradient use</th>
          <th style="text-align:left; padding:8px; border-bottom:1px solid rgba(0,0,0,0.12)">Typical use cases</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Nested EA</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Direct EA solve at LL</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Poor</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">No</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">General-purpose, black-box functions</td>
        </tr>
        <tr>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Single-level reduction + EA</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">KKT/MPEC-style constraints</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Moderate</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Yes*</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Convex/smooth LL; structured constrained forms</td>
        </tr>
        <tr>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Co-evolutionary EAs</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Two interacting populations</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Moderate</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">No</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Strategic/competitive interaction; multiple LL optima</td>
        </tr>
        <tr>
          <td style="padding:8px;">Surrogate-assisted EA</td>
          <td style="padding:8px;">Surrogate of LL or response mapping</td>
          <td style="padding:8px;">High</td>
          <td style="padding:8px;">No/Optional</td>
          <td style="padding:8px;">Expensive LL evaluations; simulator-driven BLO</td>
        </tr>
      </tbody>
    </table>
  </div>
  <p class="muted small">*: only when required by the reformulation.</p>
</div></div>


    <div class="card">
  <h2 id="ml">ML for BLO problems</h2>

  <p>
    Recent advances in machine learning (ML) have reshaped bilevel optimization (BLO), especially in large-scale, data-driven settings.
    Unlike many evolutionary approaches that treat the lower-level (LL) solver as a black box, ML-based BLO often leverages
    <strong>differentiable programming</strong>, <strong>automatic differentiation</strong>, and <strong>gradient-informed updates</strong>.
    This shift is driven by (i) the rise of differentiable optimization tools, (ii) the ubiquity of BLO in modern ML pipelines (e.g., hyperparameter tuning,
    meta-learning, and neural architecture search), and (iii) the high computational cost of nested evolutionary evaluations at scale.
  </p>

  <p class="muted small">
    At a high level, ML workflows naturally induce bilevel structure: the UL represents a generalization-driven objective (e.g., validation performance),
    while the LL represents training/adaptation (e.g., ERM, inner-loop learning, or critic fitting).
  </p>

  <hr/>

  <h3>Supervised learning-based methods</h3>
  <p>
    Supervised learning has enabled scalable and theoretically grounded BLO solvers, particularly for hyperparameter optimization (HO),
    model selection, and meta-learning. Early work differentiated through the entire training trajectory to obtain gradients of hyperparameters
    <a href="#ref-10.5555-3045118.3045343" class="cite">[3]</a>.
    More recent methods reduce nested-loop overhead via single-timescale or single-loop stochastic updates (e.g., STABLE)
    <a href="#ref-chen2022stable" class="cite">[4]</a>,
    and improve efficiency and convergence using implicit-gradient approximations (e.g., accelerated AID)
    <a href="#ref-chen2023aid" class="cite">[5]</a>.
    Value-function-based reformulations provide an alternative route that can avoid expensive hypergradient computation and better handle constraints
    <a href="#ref-10210699" class="cite">[6]</a>.
  </p>

  <ul class="bullets">
    <li><strong>Key advantage vs EAs:</strong> gradient efficiency and scalability on large datasets / parameter spaces.</li>
    <li><strong>Main assumption:</strong> sufficient smoothness/regularity to support stable differentiation through (approximate) LL solutions.</li>
  </ul>

  <hr/>

  <h3>Deep learning and meta-learning perspectives</h3>

  <h4>Generalized bilevel learning over structured spaces</h4>
  <p>
    Beyond finite-dimensional parameter tuning, recent research extends BLO to structured and (effectively) infinite-dimensional settings.
    In <em>functional BLO</em>, the LL optimizes over a function space (e.g., a Hilbert space), which can preserve convexity in the inner problem while
    enabling expressive hypothesis classes
    <a href="#ref-petrulionyte2024functional" class="cite">[7]</a>.
  </p>

  <div class="equation"><div class="math">
\begin{equation}\label{eq:ml-functional-blo}
h^*_{\omega} = \arg\min_{h\in\mathcal{H}} \ \mathbb{E}_{(\mathbf{x},\mathbf{y})}\!\left[\|y-h(\mathbf{x})\|^2\right] + \frac{\omega}{2}\|h\|^2_{\mathcal{H}}.
\end{equation}
  </div></div>

  <p>
    Similarly, graph learning has been interpreted through BLO lenses, where message passing can be viewed as (approximate) descent steps of an LL process and
    UL objectives guide global learning goals
    <a href="#ref-zheng2024graph" class="cite">[8]</a>.
    In structured decision-making and control, gradient-based frameworks combine implicit differentiation ideas with policy-gradient updates when UL gradients are not directly available
    <a href="#ref-banker2025gradient" class="cite">[9]</a>.
  </p>

  <hr/>

  <h3>Differentiable optimization in bilevel learning</h3>
  <p>
    Differentiable BLO is central to modern ML pipelines because UL learning requires gradients through the LL solution map.
    Two dominant strategies are: <strong>(i) implicit differentiation</strong> and <strong>(ii) unrolled optimization</strong>
    <a href="#ref-landry2021differentiable" class="cite">[10]</a>.
  </p>

  <h4>Implicit differentiation</h4>
  <p>
    If the LL optimum \(\mathbf{x}^*(\mathbf{y})\) satisfies a stationarity condition (or KKT conditions),
    hypergradients can be computed without unrolling all LL iterations. A common template is:
  </p>

  <div class="equation"><div class="math">
\begin{equation}\label{eq:ml-implicit-hypergradient}
\nabla_{\mathbf{y}} F(\mathbf{x}^*(\mathbf{y}),\mathbf{y})
\approx
\nabla_{\mathbf{y}} F
-
\nabla^2_{\mathbf{x}\mathbf{y}} f \, \left(\nabla^2_{\mathbf{x}\mathbf{x}} f\right)^{-1}\! \nabla_{\mathbf{x}} F,
\end{equation}
  </div></div>

  <p class="muted small">
    Practical note: many large-scale methods avoid forming Hessians explicitly and instead use vector-Jacobian/Hessian-vector products and iterative linear solvers
    <a href="#ref-franceschi2018bilevel" class="cite">[11]</a>.
  </p>

  <h4>Unrolled optimization</h4>
  <p>
    Unrolling treats the LL solver as a differentiable computation graph. For example, if LL uses \(T\) steps of gradient descent:
  </p>

  <div class="equation"><div class="math">
\begin{equation}\label{eq:ml-unrolled-gd}
\mathbf{x}_{t+1} = \mathbf{x}_{t} - \eta \nabla_{\mathbf{x}} f(\mathbf{x}_{t},\mathbf{y}), \quad t=0,1,\ldots,T-1,
\end{equation}
  </div></div>

  <p class="muted small">
    Unrolling is flexible and often “faithful” to the chosen LL optimizer, but memory/time scale with \(T\), which can be costly in deep networks.
  </p>

  <hr/>

  <h3>Hyperparameter optimization and meta-learning</h3>
  <p>
    A standard HO bilevel formulation uses validation loss at the UL and training loss at the LL:
  </p>

  <div class="equation"><div class="math">
\begin{equation}\label{eq:ml-hpo}
\min_{\lambda}\ \mathcal{L}_{\text{val}}(\theta^*(\lambda))
\quad \text{s.t.}\quad
\theta^*(\lambda)=\arg\min_{\theta}\ \mathcal{L}_{\text{train}}(\theta,\lambda).
\end{equation}
  </div></div>

  <p>
    This viewpoint unifies HO and meta-learning (“learning to learn”), where the LL performs task adaptation and the UL enforces generalization across tasks
    <a href="#ref-franceschi2017bridge" class="cite">[12]</a>.
    Practical scalable first-order methods have been proposed for nonconvex bilevel learning (e.g., BOme!)
    <a href="#ref-liu2022bome" class="cite">[13]</a>.
  </p>

  <hr/>

  <h3>Neural architecture search</h3>
  <p>
    Differentiable NAS is naturally bilevel: the LL trains weights on training data, while the UL updates architecture parameters using validation performance.
    DARTS-style methods relax discrete architectures into continuous mixtures via softmax weights
    <a href="#ref-liu2018darts" class="cite">[14]</a>.
  </p>

  <div class="equation"><div class="math">
\begin{equation}\label{eq:ml-darts-softmax}
m_{ij}(\cdot)=\sum_{m\in\mathcal{M}}\frac{\exp(\alpha_{ij}^{m})}{\sum_{m'\in\mathcal{M}}\exp(\alpha_{ij}^{m'})}\, m(\cdot),
\end{equation}
  </div></div>

  <p class="muted small">
    Many follow-ups focus on stability and reducing second-order cost (e.g., Neumann-series approximations)
    <a href="#ref-10440128" class="cite">[15]</a>.
  </p>

  <hr/>

  <h3>Adversarial ML</h3>
  <p>
    Adversarial learning problems often have bilevel or min–max structure. In GAN-style training, the discriminator and generator optimize coupled objectives:
  </p>

  <div class="equation"><div class="math">
\begin{equation}\label{eq:ml-gan-minmax}
\min_{\mathbf{y}}\ \max_{\mathbf{x}}\ V(\mathbf{x},\mathbf{y})
=
\mathbb{E}_{u\sim p_{\text{data}}}[\log D_{\mathbf{x}}(u)]
+
\mathbb{E}_{v\sim \mathcal{N}(0,1)}[\log(1-D_{\mathbf{x}}(G_{\mathbf{y}}(v)))].
\end{equation}
  </div></div>

  <p>
    BLO viewpoints help explain stability mechanisms (e.g., unrolling, regularization, bilevel tuning of adversarial components) and motivate hybrid schemes
    that learn response mappings or surrogate the LL
    <a href="#ref-10185941" class="cite">[16]</a>.
  </p>

  <hr/>

  <h3>Summary table</h3>
  <div style="overflow:auto">
    <table style="width:100%; border-collapse:collapse">
      <thead>
        <tr>
          <th style="text-align:left; padding:8px; border-bottom:1px solid rgba(0,0,0,0.12)">Technique</th>
          <th style="text-align:left; padding:8px; border-bottom:1px solid rgba(0,0,0,0.12)">Main idea</th>
          <th style="text-align:left; padding:8px; border-bottom:1px solid rgba(0,0,0,0.12)">Use cases</th>
          <th style="text-align:left; padding:8px; border-bottom:1px solid rgba(0,0,0,0.12)">Refs</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Differentiable BLO</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Implicit differentiation or unrolled optimization for hypergradients</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">HO, NAS, meta-learning</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">
            <a href="#ref-franceschi2017bridge" class="cite">[12]</a>,
            <a href="#ref-liu2022bome" class="cite">[13]</a>,
            <a href="#ref-10440128" class="cite">[15]</a>
          </td>
        </tr>
        <tr>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Supervised bilevel solvers</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Single-loop / implicit-gradient approximations with guarantees</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Tuning, meta-learning, constrained BLO</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">
            <a href="#ref-chen2022stable" class="cite">[4]</a>,
            <a href="#ref-chen2023aid" class="cite">[5]</a>,
            <a href="#ref-10210699" class="cite">[6]</a>
          </td>
        </tr>
        <tr>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Graph / functional BLO</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">Bilevel learning over structured spaces and response processes</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">GNNs, IV regression, structured learning</td>
          <td style="padding:8px; border-bottom:1px solid rgba(0,0,0,0.08)">
            <a href="#ref-petrulionyte2024functional" class="cite">[7]</a>,
            <a href="#ref-zheng2024graph" class="cite">[8]</a>
          </td>
        </tr>
        <tr>
          <td style="padding:8px;">Adversarial BLO views</td>
          <td style="padding:8px;">Min–max / bilevel modeling for stability and response approximation</td>
          <td style="padding:8px;">GANs, robustness, poisoning/attacks</td>
          <td style="padding:8px;">
            <a href="#ref-10185941" class="cite">[16]</a>
          </td>
        </tr>
      </tbody>
    </table>
  </div>

  <hr/>

  <h3>Synthesis: when do ML-based bilevel solvers work best?</h3>
  <p class="muted">
    In practice, the dominant success factor is the <strong>geometry/regularity of the LL problem</strong>.
    Implicit differentiation is most reliable when the LL solution is locally unique and sufficiently smooth; unrolling is more flexible but costs memory/time
    proportional to LL iterations. Differentiable optimization layers work best when LL structure is known (e.g., convex programs),
    while first-order single-loop or penalty-style methods trade some accuracy for stability in nonconvex LL settings.
    A practical rule is to choose the solver family primarily by LL regularity (smooth/convex vs. nonsmooth/nonconvex vs. black-box),
    and only then by scale and compute constraints.
  </p>
</div>

      <div class="card">
  <h2 id="hybrid">Hybrid EA + ML methods</h2>

  <p>
    Rather than viewing ML and EAs as competing paradigms, a growing body of work integrates them to exploit complementary strengths:
    EAs offer robustness and global exploration in rugged/discrete/black-box landscapes, while ML offers data efficiency, generalization, and gradient-informed guidance.
    Hybrid designs are especially attractive when LL evaluations are expensive, LL solutions are non-unique, or the search space is combinatorial/mixed-integer.
  </p>

  <hr/>

  <h3>Common hybrid patterns</h3>
  <ul class="bullets">
    <li>
      <strong>EA + learned surrogate:</strong> learn LL objective/constraints or UL→LL response mapping to reduce nested evaluation cost
      <a href="#ref-dumouchelle2024neur2bilo" class="cite">[17]</a>.
    </li>
    <li>
      <strong>Learning-guided reformulation:</strong> learn components that enable reformulating a hard bilevel model into a tractable hybrid optimization pipeline
      <a href="#ref-bagloee2018hybrid" class="cite">[18]</a>.
    </li>
    <li>
      <strong>Neural surrogate embedded in exact solvers:</strong> train a predictor (value function / feasibility / response) and embed it inside MIP/MINLP pipelines
      <a href="#ref-dumouchelle2024neur2bilo" class="cite">[17]</a>.
    </li>
    <li>
      <strong>Hybrid learning + heuristic LL:</strong> UL uses learning to propose structure changes; LL uses problem-specific heuristics to solve the resulting combinatorial instance
      <a href="#ref-wang2021bi" class="cite">[19]</a>.
    </li>
    <li>
      <strong>Generative modeling for LL response sets:</strong> learn a distribution over LL solutions (set-valued mapping) and only verify promising candidates with the true LL solver
      <a href="#ref-10185941" class="cite">[16]</a>.
    </li>
  </ul>

  <hr/>

  <h3>Representative examples (website-friendly)</h3>
  <p class="muted small">
    These examples illustrate “division of labor”: ML reduces evaluation cost or guides proposals; EA/heuristics preserve robustness in discrete or rugged spaces.
  </p>

  <ul class="bullets">
    <li>
      <strong>Learning-guided transportation network design:</strong> learn mappings that approximate LL behaviors to reduce computational burden and enable tractable optimization
      <a href="#ref-bagloee2018hybrid" class="cite">[18]</a>.
    </li>
    <li>
      <strong>Neur2BiLO:</strong> supervised neural approximation of leader/follower value functions embedded into mixed-integer optimization for faster bilevel solving
      <a href="#ref-dumouchelle2024neur2bilo" class="cite">[17]</a>.
    </li>
    <li>
      <strong>Bilevel learning for FFNN design:</strong> UL searches structure (e.g., binary PSO) while LL optimizes weights (e.g., LM), balancing complexity vs. generalization
      <a href="#ref-9212586" class="cite">[20]</a>.
    </li>
    <li>
      <strong>Multiobjective combinatorial BLO:</strong> indicator-based co-evolution prioritizes LL solutions that most improve UL progress under expensive LL Pareto evaluations
      <a href="#ref-9154441" class="cite">[21]</a>.
    </li>
  </ul>

  <hr/>

  <h3>Practical guidance</h3>
  <ul class="bullets">
    <li><strong>If LL is expensive:</strong> prefer surrogate-assisted or response-learning hybrids with uncertainty-aware verification.</li>
    <li><strong>If LL is discrete/combinatorial:</strong> use ML for guidance (ranking, candidate generation), but keep EA/heuristics for feasibility and robustness.</li>
    <li><strong>If LL is smooth/convex:</strong> differentiable ML methods may dominate; hybrids help mainly when constraints/discreteness break differentiability.</li>
  </ul>

  <p class="muted small">
    Overall takeaway: ML enhances EA pipelines by reducing LL cost and improving guidance, while EAs/heuristics provide robustness in settings
    where gradients are unreliable or unavailable.
  </p>
</div>

    </div>

    <div class="card" id="example">
      <h2>Example: Toll setting in transportation systems</h2>
      <p class="muted small">
        Adapted from a common illustrative example (you can keep a footnote-style link for the public explanation).
      </p>

      <p>
        A government chooses toll values \(\mathbf{x}\) to maximize toll revenue \(F(\mathbf{x}, \mathbf{y})\), where \(\mathbf{y}\) denotes the resulting traffic flow.
        Travelers respond by selecting routes that minimize their travel cost \(f(\mathbf{x}, \mathbf{y})\) (including tolls and travel time).
        The UL models revenue maximization, while the LL captures user equilibrium / route choice under network constraints.
        This illustrates the nested leader–follower structure typical of BLO.
      </p>

      <p class="muted small">
        Public explainer link:
        <a class="link" href="https://en.wikipedia.org/wiki/Bilevel_optimization" target="_blank" rel="noreferrer">Wikipedia: Bilevel optimization</a>
      </p>
    </div>

    <section class="container section" id="references" style="padding:0">
      <div class="card">
        <h2>References</h2>
        <ol class="refs">
          <li id="ref-7942105">
            A. Sinha, P. Malo, and K. Deb, “A Review on Bilevel Optimization: From Classical to Evolutionary Approaches and Applications,”
            <i>IEEE Transactions on Evolutionary Computation</i>, vol. 22, no. 2, pp. 276–295, 2018, doi: 10.1109/TEVC.2017.2712906.
          </li>

          <li id="ref-zhang2023introductionbileveloptimizationfoundations">
            Y. Zhang et al., “An introduction to bilevel optimization: Foundations and applications in signal processing and machine learning,”
            <i>IEEE Signal Processing Magazine</i>, vol. 41, no. 1, pp. 38–59, 2024.
          </li>

          <!-- Added for "Classical methods" -->
          <li id="ref-anandalingam1992hierarchical">
            G. Anandalingam and T. Friesz, “Hierarchical optimization: an introduction,” <i>Annals of Operations Research</i>, vol. 34, pp. 1–11, 1992.
          </li>

          <li id="ref-colson2007overview">
            B. Colson, P. Marcotte, and G. Savard, “An overview of bilevel optimization,” <i>Annals of Operations Research</i>, vol. 153, pp. 235–256, 2007.
          </li>

          <li id="ref-bard1998practical">
            J. F. Bard, <i>Practical Bilevel Optimization: Algorithms and Applications</i>, Kluwer Academic Publishers, 1998.
          </li>

          <li id="ref-ghadimi2018approximation">
            S. Ghadimi and M. Wang, “Approximation Methods for Bilevel Programming,” arXiv:1802.02246, 2018.
          </li>

          <li id="ref-aghasi2025fully">
            A. Aghasi and S. Ghadimi, “Fully Zeroth-Order Bilevel Programming via Gaussian Smoothing,” <i>Journal of Optimization Theory and Applications</i>, vol. 205, no. 2, 2025.
          </li>

          <li id="ref-aghasi2025optimal">
            A. Aghasi, J. Umenberger, M. Alizadeh, H. Sun, and Z. Lu, “Optimal Zeroth-Order Bilevel Optimization,” arXiv:2510.03646, 2025.
          </li>

          <li id="ref-lu2023penalty">
            Z. Lu and S. Mei, “First-Order Penalty Methods for Bilevel Optimization,” arXiv:2311.07491, 2023.
          </li>

          <li id="ref-aiyoshi1984penalty">
            E. Aiyoshi and K. Shimizu, “A solution method for the static constrained Stackelberg problem via penalty method,”
            <i>IEEE Transactions on Automatic Control</i>, vol. 29, pp. 1111–1114, 1984.
          </li>

          <li id="ref-ishizuka1992double">
            Y. Ishizuka and E. Aiyoshi, “Double penalty method for bilevel optimization problems,” <i>Annals of Operations Research</i>, vol. 34, pp. 73–88, 1992.
          </li>

          <li id="ref-yuan2015trust">
            Y. x. Yuan, “Recent advances in trust region algorithms,” <i>Mathematical Programming</i>, Ser. B, vol. 151, pp. 249–281, 2015.
          </li>

          <li id="ref-colson2005trustregion">
            B. Colson, P. Marcotte, and G. Savard, “A trust region method for nonlinear bilevel programming,”
            <i>Operations Research Letters</i>, vol. 33, no. 2, pp. 141–149, 2005.
          </li>

          <li id="ref-turgut2023systematic">
  Turgut, Oguz Emrah and Turgut, Mert Sinan and K{\i}rtepe, Erhan, “A systematic review of the emerging metaheuristic algorithms on solving complex optimization problems," <i>Neural Computing and Applications</i>, vol. 35, no. 19, pp. 14275-14378, 2023, doi: 10.1007/s00521-023-08481-5.
</li>

<li id="ref-chauhan2024competitive">
  D. Chauhan, Shivani, and R. Cheng, “Competitive swarm optimizer: a decade survey,” <i>Swarm and Evolutionary Computation</i>, vol. 87, p. 101543, 2024, publisher: Elsevier, doi: 10.1016/j.swevo.2024.101543.
</li>

<li id="ref-10121701">
  Jesús-Adolfo Mejía-De-Dios, Alejandro Rodríguez-Molina, and Efrén Mezura-Montes, “Multiobjective Bilevel Optimization: A Survey of the State-of-the-Art,” <i>IEEE Transactions on Systems, Man, and Cybernetics: Systems</i>, vol. 53, no. 9, pp. 5478-5490, 2023, doi: 10.1109/TSMC.2023.3271125.
</li>

<li id="ref-sinha2014finding">
  A. Sinha, P. Malo, A. Frantsev, and K. Deb, “Finding optimal strategies in a multi-period multi-leader-follower Stackelberg game using an evolutionary algorithm,” <i>Computers & Operations Research</i>, vol. 41, pp. 374-385, 2014, doi: 10.1016/j.cor.2013.07.010.
</li>

<li id="ref-kuo2011hybrid">
  R.J. Kuo and Y.S. Han, “A hybrid of genetic algorithm and particle swarm optimization for solving bi-level linear programming problem--A case study on supply chain model,” vol. 35, no. 8, pp. 3905-3917, 2011, doi: https://doi.org/10.1016/j.apm.2011.02.008.
</li>

<li id="ref-li2024bridging">
  Li Pengyi, Jianye Hao, Hongyao Tang, Xian Fu, Yan Zheng, and Ke Tang, “Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey on Hybrid Algorithms,” ArXiv: 2401.11963, 2024, link: arxiv.org/abs/2401.11963.
</li>

<li id="ref-acm-3638530-3648409">
  <strong>“(ACM paper),” doi: 10.1145/3638530.3648409.</strong>
</li>

<li id="ref-li2023ga">
  Li et al., “(GA for single-level reduced bilevel problems),” 2023. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-camacho2022tabu">
  Camacho et al., “(Tabu/metaheuristic for reduced bilevel formulation),” 2022. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-10899879">
  “(IEEE paper, document ID 10899879),” 2024/2025. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-yang2024bilevel">
  Yang et al., “(Bilevel reformulation survey / discussion),” 2024. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-lachhwani2022comprehensive">
  Lachhwani et al., “(Comprehensive review on bilevel/assumptions/constraint qualifications),” 2022. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-chaabani2018new">
  Chaabani et al., “(Co-evolutionary bilevel EA; E-CODBA or related),” 2018. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-8425468">
  “(IEEE paper, document ID 8425468),” 2018. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-islam2017surrogate">
  Islam et al., “(Surrogate-assisted evolutionary bilevel optimization),” 2017. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-dempe2002foundations">
  S. Dempe, <i>Foundations of Bilevel Programming</i>, 2002. (Add publisher / ISBN / link)
</li>

<li id="ref-sinha2017_bleaq">
  A. Sinha et al., “BLEAQ: Bilevel evolutionary algorithm based on quadratic approximations,” <i>European Journal of Operational Research</i>, 2017.
</li>

<li id="ref-8388227">
  He et al., “(Bilevel CMA-ES with distribution sharing),” 2018. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-9476019">
  Chen et al., “TLEA (transfer-learning-based parallel bilevel EA),” 2021. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-9721406">
  “BLMOCC (bilevel multiobjective cooperative coevolution),” 2022. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-10.5555-3045118.3045343">
  D. Maclaurin, D. Duvenaud, and R. Adams, “Gradient-based Hyperparameter Optimization through Reversible Learning,”
  in <i>ICML</i>, 2015, (dblp / proceedings id: 10.5555/3045118.3045343).
</li>

<li id="ref-chen2022stable">
  Chen et al., “STABLE: single-timescale stochastic bilevel optimization,” 2022. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-chen2023aid">
  Chen et al., “Accelerated approximate implicit differentiation for bilevel optimization,” 2023. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-10210699">
  Liu et al., “BVFSM: value-function-based sequential minimization for bilevel optimization,” 2023/2024. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-petrulionyte2024functional">
  Petrulionytė et al., “Functional bilevel optimization,” 2024. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-zheng2024graph">
  Zheng et al., “Graph learning via bilevel optimization (unified viewpoint),” 2024. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-banker2025gradient">
  Banker et al., “Gradient-based bilevel framework with implicit differentiation and RL,” 2025. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-landry2021differentiable">
  Landry et al., “Differentiable optimization for ML / bilevel learning,” 2021. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-franceschi2018bilevel">
  Franceschi et al., “Bilevel programming for hyperparameter optimization and meta-learning,” 2018. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-franceschi2017bridge">
  Franceschi et al., “(Bridge between hyperparameter optimization and meta-learning via bilevel),” 2017. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-liu2022bome">
  Liu et al., “BOme!: first-order bilevel optimization for large-scale nonconvex learning,” 2022. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-liu2018darts">
  H. Liu, K. Simonyan, and Y. Yang, “DARTS: Differentiable Architecture Search,” 2018. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-10440128">
  “STO-DARTS / Neumann-series hypergradient approximation,” 2023/2024 (paper id: 10440128). (Add full bibliographic details / DOI / link)
</li>

<li id="ref-10185941">
  “(cGAN / adversarial bilevel surrogate mapping),” paper id: 10185941. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-dumouchelle2024neur2bilo">
  Dumouchelle et al., “Neur2BiLO: neural bilevel optimization embedded in MIP,” 2024. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-bagloee2018hybrid">
  Bagloee et al., “Hybrid learning + optimization for bilevel transportation network design,” 2018. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-wang2021bi">
  Wang et al., “Learning-guided bilevel framework with heuristic LL solver,” 2021. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-9212586">
  “Bilevel learning for self-organizing FFNNs (binary PSO + LM),” paper id: 9212586. (Add full bibliographic details / DOI / link)
</li>

<li id="ref-9154441">
  “IB-CEMBA: indicator-based co-evolutionary migration-based bilevel algorithm,” paper id: 9154441. (Add full bibliographic details / DOI / link)
</li>


          
        </ol>
      </div>
    </section>

    <footer class="footer">
      <div class="container">
        <div class="muted">© <span id="year"></span> BLO Hub · Built on GitHub Pages</div>
      </div>
    </footer>
  </main>

  <script src="assets/app.js"></script>
</body>
</html>

